dataset_name: Multi-Domain-Expert-Layers/arxiv
model_name_or_path: EleutherAI/pythia-1b-deduped
output_dir: "ckpts/pythia-1b-deduped/uspto/layer_9,10,11,12,13"
training_layers: "4,5"
per_device_train_batch_size: 32
per_device_eval_batch_size: 20
preprocessing_num_workers: 32
learning_rate: 0.0001
block_size: 512
num_train_epochs: 1
gradient_accumulation_steps: 1
do_train: true
do_eval: true
evaluation_strategy: "steps"
save_total_limit: 2
max_grad_norm: 2.0
save_steps: 500
overwrite_output_dir: true
logging_steps: 20
deepspeed: "configs/zero_config.json"
fp16: true
wandb_entity: "ontocord"
wandb_project: "layer-experts"
